{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 04\n",
        "The 1980s saw a shift from Natural Language Processing techniques aiming to codify the grammatical rules of natural language towards techniques aiming to use statistical models to generate text. One early idea which technically isn’t “AI” seeing as it is “memorizing” the training data and yet introduces us to the power contained in statistical techniques of text generation is the idea of Markov chains. Write a python function generate(filename: str, start_words: list[str], chain_length: int, num_generated: int) -> str which takes a filename, a chain length, a list of start words which has to be exactly as long as the chain_length (why?), and an integer num_generated and returns a sentence num_generated words long which sounds similar to the text contained in filename."
      ],
      "metadata": {
        "id": "bat5AK4jI_KO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8bmumofAt1V",
        "outputId": "5eaff886-e79d-482c-a850-0ebb977232f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The park was a place of\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "def generate(filename: str, start_words: list[str], chain_length: int, num_generated: int) -> str:\n",
        "  \"\"\"Generates a sentence of specified length based on a Markov chain created from a file.\n",
        "\n",
        "  Args:\n",
        "      filename: The path to the file containing text for training the Markov chain.\n",
        "      start_words: A list of words to start the sentence with.\n",
        "      chain_length: The length of the Markov chain (number of words to consider).\n",
        "      num_generated: The number of words to generate.\n",
        "\n",
        "  Returns:\n",
        "      A sentence of length num_generated, starting with start_words, that imitates the style of the text in the file.\n",
        "  \"\"\"\n",
        "\n",
        "  # Read text from the file\n",
        "  with open(filename, \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "  # Create a dictionary to store word transitions\n",
        "  transitions = {}\n",
        "  words = text.split()\n",
        "  for i in range(len(words) - chain_length):\n",
        "    current_sequence = \" \".join(words[i:i+chain_length])\n",
        "    next_word = words[i + chain_length]\n",
        "    if current_sequence not in transitions:\n",
        "      transitions[current_sequence] = []\n",
        "    transitions[current_sequence].append(next_word)\n",
        "\n",
        "  # Generate the sentence\n",
        "  sentence = start_words.copy()\n",
        "  for i in range(num_generated):\n",
        "    current_sequence = \" \".join(sentence[-chain_length:])\n",
        "    if current_sequence not in transitions:\n",
        "      # Handle unseen sequences (e.g., randomly choose a word)\n",
        "      break\n",
        "    next_word = random.choice(transitions[current_sequence])\n",
        "    sentence.append(next_word)\n",
        "\n",
        "  return \" \".join(sentence)\n",
        "\n",
        "# Example usage\n",
        "start_words = [\"The\"]\n",
        "chain_length = 1\n",
        "num_generated = 5\n",
        "sentence = generate(\"/content/sample_data/sample_test .txt\", start_words, chain_length, num_generated)\n",
        "print(sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code to generate multiple possible strings"
      ],
      "metadata": {
        "id": "nSrRlf9uJD0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_all(filename: str, start_words: list[str], chain_length: int, max_paths=100, max_length=20):\n",
        "  \"\"\"Generates a list of possible sentence completions based on a Markov chain.\n",
        "\n",
        "  Args:\n",
        "      filename: The path to the file containing text for training the Markov chain.\n",
        "      start_words: A list of words to start the sentence with.\n",
        "      chain_length: The length of the Markov chain (number of words to consider).\n",
        "      max_paths: The maximum number of paths to explore (stopping criteria).\n",
        "      max_length: The maximum length for generated sentences (stopping criteria).\n",
        "\n",
        "  Returns:\n",
        "      A list of strings, each representing a possible sentence completion based on the starting words.\n",
        "  \"\"\"\n",
        "\n",
        "  def build_transitions(text, chain_length):\n",
        "    \"\"\"Builds a dictionary to store word transitions.\n",
        "\n",
        "    Args:\n",
        "        text: The text to use for building the Markov chain.\n",
        "        chain_length: The length of the Markov chain (number of words to consider).\n",
        "\n",
        "    Returns:\n",
        "        A dictionary mapping sequences of words to a list of possible next words.\n",
        "    \"\"\"\n",
        "    transitions = {}\n",
        "    words = text.split()\n",
        "    for i in range(len(words) - chain_length):\n",
        "      current_sequence = \" \".join(words[i:i+chain_length])\n",
        "      next_word = words[i + chain_length]\n",
        "      if current_sequence not in transitions:\n",
        "        transitions[current_sequence] = []\n",
        "      transitions[current_sequence].append(next_word)\n",
        "    return transitions\n",
        "\n",
        "  def dfs_generate(current_sentence, explored_paths):\n",
        "    if len(current_sentence) >= max_length or explored_paths >= max_paths:\n",
        "      return\n",
        "\n",
        "    current_sequence = \" \".join(current_sentence[-chain_length:])\n",
        "    if current_sequence not in transitions:\n",
        "      return\n",
        "\n",
        "    for next_word in transitions[current_sequence]:\n",
        "      new_sentence = current_sentence.copy()\n",
        "      new_sentence.append(next_word)\n",
        "      dfs_generate(new_sentence, explored_paths + 1)\n",
        "      all_outputs.append(\" \".join(new_sentence))\n",
        "\n",
        "  transitions = build_transitions(open(filename, \"r\").read(), chain_length)  # Helper function to build the dictionary\n",
        "  all_outputs = []\n",
        "  dfs_generate(start_words.copy(), 0)\n",
        "  return all_outputs\n",
        "\n",
        "# Example usage\n",
        "start_words = [\"loved\", \"to\"]\n",
        "chain_length = 2\n",
        "max_paths = 50\n",
        "max_length = 10\n",
        "all_sentences = generate_all(\"/content/sample_data/sample_test .txt\", start_words, chain_length, max_paths, max_length)\n",
        "print(f\"Number of generated sentences: {len(all_sentences)}\")\n",
        "print(\"Sample sentences:\")\n",
        "for sentence in all_sentences[:5]:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfOIgGDbFHoe",
        "outputId": "0d742416-b557-4903-adf1-511bcf2c2fbf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of generated sentences: 20\n",
            "Sample sentences:\n",
            "loved to read. She loved to read. She loved to\n",
            "loved to read. She loved to read. She loved\n",
            "loved to read. She loved to read. She\n",
            "loved to read. She loved to read.\n",
            "loved to read. She loved to read books, magazines, and\n"
          ]
        }
      ]
    }
  ]
}
